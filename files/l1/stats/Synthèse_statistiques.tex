\documentclass{article}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{graphicx}

\begin{document}

\title{Synthèse du cours de Statistiques}
\author{}
\date{}
\maketitle

\section*{Statistiques univariées}

\subsection*{Vocabulaire et classification des variables}
\begin{itemize}
    \item \textbf{Variable discrète} : prend un nombre dénombrable de valeurs (exemple : nombre d’élèves ayant une note donnée).
    \item \textbf{Variable continue} : peut prendre toutes les valeurs dans un intervalle (exemple : poids mesuré entre 60 kg et 70 kg).
    \item \textbf{Variables qualitatives et quantitatives} : 
    \begin{itemize}
        \item Qualitative : regroupe des modalités non numériques (exemple : couleurs).
        \item Quantitative : associée à des valeurs numériques.
    \end{itemize}
\end{itemize}

\subsection*{Représentation des données}
\begin{itemize}
    \item \textbf{Diagramme en bâtons} : pour les variables discrètes.
    \item \textbf{Histogramme} : pour les variables continues, où l’aire des rectangles correspond à l’effectif.
    \item \textbf{Fonction de répartition} : permet de répondre à des questions comme "combien d’élèves ont une note inférieure ou égale à 4 ?".
\end{itemize}

\subsection*{Caractéristiques de position}
\begin{itemize}
    \item \textbf{Mode} : valeur la plus fréquente

(Exemple : 

Supposons qu'on ait les notes de 10 élèves :
4, 6, 7, 4, 8, 4, 5, 6, 7, 4,

La note 4 est répétée 4 fois.
Les autres notes (6, 7, 8, 5) sont moins fréquentes
: 4 est le mode de cet ensemble de données).
    \item \textbf{Médiane} : valeur qui sépare la série en deux parties égales. Exemple : si 10 élèves passent un examen, la médiane est la moyenne des notes des 5e et 6e élèves.
    \item \textbf{Moyenne} : somme des valeurs divisée par leur nombre. Exemple :
    \[
    \bar{x} = \frac{\sum (x_i \cdot n_i)}{N}.
    \]
\end{itemize}

\subsection*{Caractéristiques de dispersion}
\begin{itemize}
    \item \textbf{Étendue} : différence entre la valeur maximale et minimale.
    \item \textbf{Variance et écart-type} : 
    \[
    \text{Var}(x) = \frac{\sum (x_i - \bar{x})^2 n_i}{N}.
    \]
    L’écart-type est la racine carrée de la variance (\(\sigma\)), indiquant l'écart moyen autour de la moyenne.
\end{itemize}

\section*{Statistiques bivariées}

\subsection*{Analyse des relations entre deux variables}
\begin{itemize}
    \item \textbf{Distributions conditionnelles et marginales} : 
    \begin{itemize}
        \item \textbf{Marginale} : répartition de chaque variable indépendamment (somme sur les lignes ou colonnes).
        \item \textbf{Conditionnelle} : répartition d’une variable en fixant une modalité de l’autre.
    \end{itemize}
\end{itemize}

\subsection*{Mesures de dépendance}
\textbf{Objectif} : vérifier si deux variables sont indépendantes ou non. La statistique du \(\chi^2\) est donnée par :
\[
D_{\chi^2} = n \sum_{j=1}^{J} \sum_{k=1}^{K} \frac{(f_{jk} - f_{j\cdot} f_{\cdot k})^2}{f_{j\cdot} f_{\cdot k}}.
\]
\subsection*{}
Les coefficients suivants mesurent la dépendance entre deux variables :
\begin{itemize}
    \item \(\phi = \sqrt{\frac{D_{\chi^2}}{n}}\) : intensité de la relation.
    \item \(V\) de Cramér : plus précis pour les tableaux non carrés.
\end{itemize}
L'interprétation de \(V\) :
\begin{itemize}
    \item \(V < 0.1\) : relation faible ou nulle.
    \item \(V > 0.3\) : relation forte.
\end{itemize}

\section*{Régression linéaire}

\subsection*{Nuage de points}
Chaque individu est représenté par un point \((x_i, y_i)\). Le \textbf{point moyen} est donné par \((\bar{x}, \bar{y})\), centre de gravité du nuage.

\subsection*{Méthode des moindres carrés}
L'objectif est de minimiser la somme des carrés des distances verticales entre les points et la droite de régression :
\[
a = \frac{\text{Cov}(x, y)}{\sigma_x^2}, \quad b = \bar{y} - a \bar{x}.
\]
La covariance est calculée par :
\[
\text{Cov}(x, y) = m(xy) - \bar{x} \bar{y}.
\]

\subsection*{Coefficient de corrélation linéaire (\(r\))}
La mesure de la relation linéaire entre \(x\) et \(y\) est donnée par :
\[
r = \frac{\text{Cov}(x, y)}{\sigma_x \sigma_y}.
\]
\(r = 1\) ou \(r = -1\) indique une relation parfaitement linéaire.

\subsection*{Coefficient de détermination (\(R^2\))}
Il mesure la proportion de la variance de \(y\) expliquée par \(x\). Ce coefficient équivaut au coefficient de corrélation linéaire au carré (r^2)
:

\[
R^2 = \frac{\text{Var}(ax+b)}{\text{Var}(y)}.
\]
\[R^2 = (r^2)\]

\section*{Tests statistiques}

\subsection*{Hypothèses et p-value}
L'\textbf{hypothèse nulle} (\(H_0\)) est la proposition initiale à tester (ex. : indépendance des variables). La \textbf{p-value} est la probabilité d’erreur si \(H_0\) est rejetée. On rejette \(H_0\) si \(p \leq \alpha\) (généralement \( \alpha = 0.05 \)).

\subsection*{Tests sur régression}
\begin{itemize}
    \item \textbf{Test t de Student} : vérifie l’effet individuel des variables explicatives.
    \item \textbf{Test F de Fisher (ANOVA)} : analyse globale de l’effet des variables explicatives.
\end{itemize}

\subsection*{Tests de normalité}
Les tests de normalité, comme le test de Shapiro-Wilk, permettent de vérifier si une série suit une loi normale.Si \(p > 0.05\), on ne rejette pas \(H_0\) et la série peut être supposée normale.



\author{}
\date{}


\maketitle

\section*{ Test du Chi-2}

Le test du Chi-2 permet de tester l’indépendance entre deux variables qualitatives à partir d’un tableau de contingence. Voici les étapes principales :

\begin{enumerate}
    \item \textbf{Hypothèse nulle (\(H_0\))} : Les deux variables sont indépendantes.
    \item \textbf{Statistique du test} : La distance du Chi-2 (\(D_{\chi^2}\)) est calculée par la formule :
    \[
    D_{\chi^2} = n \sum_{j=1}^J \sum_{k=1}^K \frac{(f_{jk} - f_{j\bullet}f_{\bullet k})^2}{f_{j\bullet}f_{\bullet k}}
    \]
    où \(f_{jk}\) est la fréquence observée, et \(f_{j\bullet}\) et \(f_{\bullet k}\) sont les marges des fréquences.
    \item \textbf{Quantile et décision} :
    \begin{itemize}
        \item On détermine \(d = (J-1)(K-1)\), le degré de liberté.
        \item Au seuil \(\alpha\) (souvent \(5\%\)), on compare \(D_{\chi^2}\) au quantile \(q_{1-\alpha}\) d’une loi \(\chi^2_d\).
        \item Si \(D_{\chi^2} \geq q_{1-\alpha}\), on rejette \(H_0\), concluant que les variables sont dépendantes.
    \end{itemize}
\end{enumerate}

\textbf{Exemple} : Avec \(D_{\chi^2} = 11.6\) pour \(d = 1\), et \(q_{0.95} = 3.84\), on conclut à une dépendance car \(11.6 > 3.84\).

\section*{ ANOVA à un Facteur}

L’ANOVA (Analysis of Variance) à un facteur évalue si une variable qualitative (facteur) explique une différence significative entre les moyennes d’une variable quantitative.

    \item \textbf{Utilisation} :
    \begin{itemize}
        \item En L1 on ne passe que par la p-value.
        La \textbf{p-value} est utilisée pour déterminer si les moyennes des différents groupes ou niveaux du facteur étudié sont statistiquement différentes les unes des autres.
        La p-value est calculée à partir du rapport entre la variance expliquée par les groupes (variance inter-groupes) et la variance résiduelle (variance intra-groupe).

\begin{itemize}
    \item Une p-value inférieure à un seuil prédéfini (souvent \( \alpha = 0,05 \)) suggère que les différences observées entre les groupes ne sont pas dues au hasard, conduisant au rejet de \( H_0 \). Cela indique qu’au moins un des groupes diffère significativement des autres.
    \item Une p-value supérieure au seuil indique qu’il n’y a pas suffisamment de preuves pour rejeter \( H_0 \), et les moyennes des groupes sont considérées comme statistiquement similaires.
\end{itemize}



    \end{itemize}
\end{enumerate}

²
\section*{}
Ces outils sont essentiels pour tester des hypothèses en statistique. Le test du Chi-2 explore les relations entre variables qualitatives, tandis que l’ANOVA examine les effets d’un facteur qualitatif sur une variable quantitative.Et l'ANOVA sur une régréssion linéaire montre l'effet d'une quantitative sur une quantitative .



\end{document}
